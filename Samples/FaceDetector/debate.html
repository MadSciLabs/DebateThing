<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<link rel="stylesheet" href="https://www.visagetechnologies.com/wp-content/themes/visage_1.3/style.css" type="text/css" media="all" />
	<link rel="icon" href="https://www.visagetechnologies.com/wp-content/themes/visage_1.3/images/visage.ico" />	
</head>
<body>
<header>
</header>

<video style="display:none" id="webcam" autoplay width="640px" height="360px"></video>

<a href="javascript:nose(0,'000');">Nose Left Small</a> | 
<a href="javascript:nose(0,'050');">Nose Left Lrg</a> | 
<a href="javascript:nose(1,'000');">Nose Right Small</a>
<a href="javascript:nose(1,'050');">Nose Right Lrg</a>

<br><br>

<div class="canvasContainer" id="container"></div>
<div class="canvasContainer" id="stub" style="width:640px;height:480px"></div>
<br><br>

<div id="emptything" style="overflow: hidden;margin-left: 100px;">
	<canvas id="canvas" width="640" height="480" style="float:left;border:1px solid black;-webkit-transform: scaleX(-1);-moz-transform: scaleX(-1);"></canvas>
</div>

<script type="text/javascript">
var Module = 
{
	onRuntimeInitialized: function()
	{
		onModuleInitialized();
	}
};
</script>


<script src="bezier-spline.js"></script>
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>

<!--AR STUFF -->
<script src="three.min.js"></script>
<script src="MTLLoader.js"></script>
<script src="OBJMTLLoader.js"></script>
<script src="ModelAR.js"></script>


<script type='text/javascript'>

	var _initNose0 = false;
	var _initNose1 = false;

	var noseSize0 = 0;
	var noseSize1 = 0;

	// AR STUFF
	var container = document.getElementById('container');
	//var webcam = document.getElementById('webcam');
	var v_ar = new VisageAR();
	var ar_initialized = false;

	//Total Noses
	var NUM_NOSES = 5;

	var mWidth = 0,mHeight = 0;


var minFaceScale = 0;
	
var canCon = canvas.getContext('2d');	
var startDetecting = false;
var draw = true;

var splineResolution = 5;


</script>

<script type="text/javascript" src="drawing_functions.js"></script>


<script type='text/javascript'>
/*
* Callback method mentioned in the documentation. 
* Gets executed after all the preparation is done (all the files have been downloaded) and tracker is ready to start tracking.
* In this case it enables buttons on the page.
*/
function callbackDownload()
{
	StartDetector();
}

var frameSample = [0,0,0,0,0];
var newSample = [0,0,0,0,0];
var ppixels,
	pixels;
var maxFaces = 10;

/*
*Method that is called on every frame via requestAnimationFrame mechanism.
*Draws camera image on the canvas, takes the pixel data, sends them to the tracker and finally, depending on the result, draws the results.
*Rudimentary timing is implemented to be activated on button click and it also checks for duplicate frames.
*/
function processFrame(){

	window.requestAnimationFrame(processFrame);

	canvas.width = mWidth;
	//Draws an image from cam on the canvas
	
	//AR STUFF
	canCon.drawImage(video,0,0,mWidth,mHeight);
	
	//Access pixel data
	imageData = canCon.getImageData(0,0, mWidth, mHeight).data;
	//imageData = v_videoContext.getImageData(0,0, mWidth, mHeight).data;

	//Save pixel data to preallocated buffer
	for(i=0; i<imageData.length; i+=1)
	{
		pixels[i] = imageData[i];
	}
	
	//Alternative way to save pixel data, seems faster but not consistent
	//pixels.set(imageData);
	
	//Call Update() if ready to start tracking and frame is new
	if (startDetecting===true)
	{
		var numOfFaces = m_Detector.detectFeatures(mWidth,mHeight,ppixels,faceData,10);
		console.log("Num faces : " + numOfFaces);

	}

	//Draw based upon data if tracker status is OK
	if (startDetecting===true && numOfFaces > 0)
	{
		console.log("here i'm detecting");

		for (var i=0; i < numOfFaces; i++)
		{
			//var faceDataInstance = faceData.get(i);

			/*
				Update Faces
			*/
			var faceDataInstance = faceData.get(i);

			if (faceDataInstance.getFaceTranslation()[0] > 0) {
				v_ar.updateNose(0,faceDataInstance);
			} else {
				v_ar.updateNose(1,faceDataInstance);
			}
		}
	}

}
//Function called when Start is clicked, tracking is resumed/started
function StartDetector(){
	
	startDetecting = true;

	//AR STUFF
	v_ar.startTracking();
}

function StopDetector(){
	
	startDetecting = false;

	//AR STUFF
	v_ar.stopTracking();
}

var m_Detector;
var faceData;
var imageData;

//var video = document.createElement('video');

/*
	AR STUFF
*/
function load(_index, _nose) {

	console.log("LOADING");
	console.log(_index);
	console.log(_nose);

	if (ar_initialized)
		v_ar.loadObject(_index, _nose);

}
			
function nose(_index,_size) {

	if (_index == 0) {

		_initNose0 = true;

		load(0, "models/" + _size);

	} else {

		_initNose1 = true;

		load(1, "models/" + _size);
	}
}



//Handlers for camera communication
//callback methods for getUserMedia : deniedStream, errorStream, startStream
//**************************************************************************

//Alerts the user when there is no camera
function deniedStream(){
	alert("Camera access denied!)");
}
//Pushes error to the console when there is error with camera access
function errorStream(e){
	if (e){
		console.error(e);
	}
}

function onModuleInitialized()
{	
	console.log("On Module Init");

	console.log("1");

	if (mWidth === 0)
	{
		setTimeout(onModuleInitialized, 100);
		return
	}

	//AR STUFF
	v_ar.initialize(container, video);
	stub = document.getElementById("stub");
	stub.style.display = "none";
	ar_initialized = true;

	console.log("2");

	//Allocate memory for image data
	ppixels = Module._malloc(mWidth*mHeight*4);
	//Create a view to the memory
	pixels = new Uint8ClampedArray(Module.HEAPU8.buffer, ppixels, mWidth*mHeight*4);

	//Allocate memory for output image data
	p_outputImageData = Module._malloc(mWidth*mHeight*3);
	//Create a view to the memory
	outputImage = new Uint8ClampedArray(Module.HEAPU8.buffer, p_outputImageData, mWidth*mHeight*3);
	
	//set up detector and licensing, valid license needs to be provided
	Module.initializeLicenseManager("/extras/907-495-332-673-403-834-024-561-827-124-065.vlc");
	//Module.initializeLicenseManager("492-209-201-108-805-336-029-013-891-329-010.vlc");
	m_Detector = new Module.VisageDetector();
	faceData = new Module.FaceDataVector();
	for (var i = 0; i < 10; i++)
	{
		faceData.push_back(new Module.FaceData());
	}

	//Use request animation frame mechanism - slower but with smoother animation
	processFrame();

}

//Is triggered when cam stream is successfully fetched
//NOTE: Can be buggy, try to increase the value from 1000ms to some higher value in that case
//function startStream(stream){
function startStream(stream) {

	//video.addEventListener('canplay', function DoStuff() {

	//	video.removeEventListener('canplay', DoStuff, true);
		
		setTimeout(function() {
		
			video.play();

			canvas.width = video.videoWidth * .5;
			canvas.height = video.videoHeight * .5;

			mWidth = video.videoWidth * .5;
			mHeight = video.videoHeight * .5;

		}, 500);
	//}, true);
		
	var domURL = window.URL || window.webkitURL;
	video.src = "debate_2.mp4";
	video.type = "video/mp4";
		
	video.play();
}

/*
*	Callback if the user allowed camera access.
*/
function startStreamEyewear(stream) {
				console.log("camera allowed");
				var domURL = window.URL || window.webkitURL;
				
			console.log("Video Width : " + video.videoWidth);

				//video.src = domURL ? domURL.createObjectURL(stream) : stream;

				video.src  =  "debate_2.mp4";
				video.type =  "video/mp4";

			canvas.width = video.videoWidth * .5;
			canvas.height = video.videoHeight * .5;
			
			mWidth = video.videoWidth * .5;
			mHeight = video.videoHeight * .5;

console.log("SIZE");
console.log(video.videoWidth);

				cameraAllowed = true;
}

/*
	This is the Eyewear method
*/

// get camera access
/*
navigator.getUserMedia_ =  navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia || navigator.msGetUserMedia;
window.URL = window.URL || window.webkitURL;

var video = document.getElementById('webcam');
try { navigator.getUserMedia_({ video: true, audio: false }, startStreamEyewear, deniedStream); } 
	catch (e) {
		try { navigator.getUserMedia_('video', startStreamEyewear, deniedStream); } 
			catch (e) {
				errorStream(e);
			}
}
*/

//Different browser support for fetching camera stream
/*
	THIS IS THE DETECTOR METHOD
*/
//EXTRA HERE
var video = document.getElementById('webcam');

window.URL = window.URL || window.webkitURL;
navigator.getUserMedia_ =  navigator.getUserMedia || navigator.webkitGetUserMedia ||
						  navigator.mozGetUserMedia || navigator.msGetUserMedia;
						  
(function() {
	var i = 0,
		lastTime = 0,
		vendors = ['ms', 'moz', 'webkit', 'o'];
	
	while (i < vendors.length && !window.requestAnimationFrame) {
		window.requestAnimationFrame = window[vendors[i] + 'RequestAnimationFrame'];
		window.cancelAnimationFrame =
		  window[vendors[i]+'CancelAnimationFrame'] || window[vendors[i]+'CancelRequestAnimationFrame'];
		i++;
	}
	if (!window.requestAnimationFrame) {
		alert("RequestAnimationFrame mechanism is not supported by this browser.");
	}
}());

//Here is where the stream is fetched

try {
	navigator.getUserMedia_({
		video: true,
		audio: false
	}, startStream, deniedStream);
	} catch (e) {
		try {
			navigator.getUserMedia_('video', startStream, deniedStream);
		} catch (e) {
			errorStream(e);
		}
	}


//video.loop = video.muted = true;
//video.autoplay = true;
//video.load();


</script>

<script src="../../lib/visageSDK.js"></script>

</body>
</html> 
